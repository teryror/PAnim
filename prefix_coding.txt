Prefix Coding:
- start with the example of text encoding
  -> "the smallest addressable unit of memory is a byte"
  -> therefore it makes sense to simply map different byte values to different
     characters => can threat text as array of characters, easy!
- save space by assigning shorter codelens to more common symbols
  -> to actually benefit, need to accept that there will be multiple symbols
     in some bytes, and some symbols will extend across byte boundaries
  -> since codelens are variable, need some way to tell symbol boundaries from
     the actual bit patterns
     -> could cook up other ways, but prolly the easiest: prefix coding!
- finding a good prefix code isn't trivial, many algorithms to do it
  -> Huffman proved his to be optimal, was used for a long time, thus the
     common misconception/misnomer that "prefix coding" == "Huffman coding"
  -> show huffman running on an example set of sym counts
  -> explain how code words are derived from the code tree (+ how to decode)
- "it's easy to see this generates a prefix code, but it's not at all
   obvious that this is optimal, that you can do no better" -> give argument
  for optimality:
  -> codelen = level of corresponding leaf node
  -> msg_len = sum(freq_i * codelen_i)
  => msg_len starts at 0, each step of tree construction increments it by the
     locally minimal amount
  -> locally non-optimal choices are either an irrelevant reordering of steps
     (i.e. same result), or an 'investment' that will pay off later
  -> the later you invest, the more unlikely you are to see it pay off, but
     the earlier you try, the more it costs
  => it's impossible to benefit from locally non-optimal choices, so the greedy
     algorithm will suffice
  -> also point out that this 'proof' depends on the Lemma that all prefix
     codes can be represented as binary trees, prove by contradiction!

Prefix Coding II (?)
- note the requirement of having the code tree in memory to decode a message,
  sending the tree is prohibitively expensive for certain size messages
  -> note that you could agree on a code tree to be used for _all_ messages
     to circumvent this issue at the cost of optimality (though some multimedia
     codecs do this!)
  -> many degrees of freedom in tree construction
     -> switching left/right child of an internal node doesn't affect codelens
     -> in fact, can arbitrarily switch nodes within the same tree level
     -> when two symbols with same freqs happen to end up with different
        codelens, can even switch those without affecting msg_len
  -> no point in trying to send codeword table instead, can always reconstruct
     full code tree from it, so exact same number of degrees of freedom
  -> trying to send symbol frequencies instead is even worse (multiple
     frequency distributions will generate same tree, so even _more_ freedom!)
  -> solution: devise a ruleset for how the code tree should be structured given
     only the codelens (since that's the minimum amount of info you need for
     to retain optimal coding!)
- that ruleset is canonical coding:
  - shuffle the code tree around such that leafs when traversed left to right
    are at non-decreasing depth; leafs at the same depth are sorted
    alphabetically
  - the left-most leaf (i.e. least frequent symbol) has to be assigned a
    codeword of all zeroes (obvious)
  - within the same depth (i.e. codelen), codewords are sequential binaries
  - at codelen boundaries, the left-most leaf _must_ be a child (or grand-child,
    etc) of what would have been the next leaf if there were no codelen boundary
    -> the sequentialy next codelen is shifted up by the difference in codelen,
       the zero-fill is correct behaviour (because the first leaf has to be the
       left-most one in the subtree)
- introduce an algorithm that produces only codelens, no full codewords ???!!
  -> will probably require the Kraft inequality: explain in terms of binary
     trees / recursive summing / exponential falloff of value as you go down
     the tree levels

Prefix Coding III (?)
- fast (table-driven) decoding -> straight forward to animate, yay!
  -> jump off into length-limited coding
- algorithm 1: moffat 3-pass + 2-pass heuristic adjustment ???!!
- algorithm 2: package-merge ???!!
  -> see that cbloom rant where he makes the connection between the two

Information Theory (Bonus Video?):
- pick up after the Kraft inequality explanation, reframe it as summing
  probabilities (i.e. making sure sym probabilities sum to 1)
  -> highlights how p=2^-b <=> -log(p)=b
  -> explain fractional bits with different radices (e.g. 2 trits ~= 3.17 bits)
     -> reminder that base-2 computing is almost as arbitrary as base-10
        (binary computers are just way easier to build with actual electronics)
  -> introduce mixing bases, e.g. in packing struct fields by mul+add
- jump off to the model/coder paradigm
  -> statistical model tries to estimate probabilites for possible symbols
  -> entropy coder tries to emit matching number of bits
- explain that prefix coding is a form of entropy coding
  -> really bad at handling changing probability distributions because of the
     requirement to calculate explicit codeword tables
  -> not actually optimal, since it can't emit fractional bits, effectively
     rounding probabilities to negative powers of two

Personal Research:
- an algorithm for finding codelens that works by sorting symbols by frequency
  and determining codelens at the same time (i.e. let yourself be inspired by
  quicksort)
